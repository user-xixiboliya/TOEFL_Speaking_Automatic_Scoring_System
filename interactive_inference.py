#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº¤äº’å¼è¯­éŸ³è¯„åˆ†ç³»ç»Ÿ
==================

åŠŸèƒ½ï¼š
1. æ˜¾ç¤ºé¢˜ç›®å’Œå¬åŠ›åŸæ–‡
2. æ’­æ”¾é¢˜ç›®éŸ³é¢‘ï¼ˆå¯é€‰ï¼‰
3. å½•åˆ¶è€ƒç”Ÿå›ç­”
4. è‡ªåŠ¨è¯„åˆ†å¹¶è¾“å‡ºç»“æœ

ä½¿ç”¨æ–¹å¼ï¼š
    python interactive_speech_scoring.py
"""

from __future__ import annotations

import os
import sys
import re
import json
import time
import wave
import tempfile
import threading
from pathlib import Path
from typing import Dict, Tuple, Optional, Any
from dataclasses import dataclass

# æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„åº“
def check_dependencies():
    """æ£€æŸ¥å¹¶æç¤ºå®‰è£…å¿…è¦çš„ä¾èµ–"""
    missing = []
    
    try:
        import torch
    except ImportError:
        missing.append("torch")
    
    try:
        import torchaudio
    except ImportError:
        missing.append("torchaudio")
    
    try:
        import pyaudio
    except ImportError:
        missing.append("pyaudio")
    
    try:
        import requests
    except ImportError:
        missing.append("requests")
    
    if missing:
        print("ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åº“ï¼Œè¯·å®‰è£…ï¼š")
        print(f"  pip install {' '.join(missing)}")
        if "pyaudio" in missing:
            print("\næ³¨æ„ï¼špyaudio å¯èƒ½éœ€è¦é¢å¤–æ­¥éª¤å®‰è£…ï¼š")
            print("  Ubuntu/Debian: sudo apt-get install portaudio19-dev python3-pyaudio")
            print("  macOS: brew install portaudio && pip install pyaudio")
            print("  Windows: pip install pyaudio")
        return False
    return True


# ============================================================================
# å½•éŸ³æ¨¡å—
# ============================================================================

class AudioRecorder:
    """
    éŸ³é¢‘å½•åˆ¶å™¨
    æ”¯æŒæŒ‰é”®å¼€å§‹/åœæ­¢å½•éŸ³
    """
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 channels: int = 1,
                 chunk_size: int = 1024,
                 format_type: int = None):
        """
        Args:
            sample_rate: é‡‡æ ·ç‡
            channels: å£°é“æ•°
            chunk_size: ç¼“å†²åŒºå¤§å°
            format_type: éŸ³é¢‘æ ¼å¼ï¼ˆpyaudio æ ¼å¼å¸¸é‡ï¼‰
        """
        import pyaudio
        
        self.sample_rate = sample_rate
        self.channels = channels
        self.chunk_size = chunk_size
        self.format_type = format_type or pyaudio.paInt16
        
        self.pyaudio = pyaudio.PyAudio()
        self.frames = []
        self.is_recording = False
        self.stream = None
    
    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        self.frames = []
        self.is_recording = True
        
        self.stream = self.pyaudio.open(
            format=self.format_type,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size
        )
        
        print("\nğŸ¤ å½•éŸ³ä¸­... æŒ‰ Enter é”®åœæ­¢å½•éŸ³")
        
        # åœ¨åå°çº¿ç¨‹ä¸­å½•éŸ³
        def record_loop():
            while self.is_recording:
                try:
                    data = self.stream.read(self.chunk_size, exception_on_overflow=False)
                    self.frames.append(data)
                except Exception as e:
                    if self.is_recording:
                        print(f"å½•éŸ³é”™è¯¯: {e}")
                    break
        
        self.record_thread = threading.Thread(target=record_loop)
        self.record_thread.start()
    
    def stop_recording(self) -> list:
        """åœæ­¢å½•éŸ³å¹¶è¿”å›éŸ³é¢‘å¸§"""
        self.is_recording = False
        
        if self.record_thread:
            self.record_thread.join(timeout=1)
        
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
            self.stream = None
        
        return self.frames
    
    def save_to_wav(self, filename: str, frames: list = None) -> str:
        """
        ä¿å­˜å½•éŸ³åˆ° WAV æ–‡ä»¶
        
        Args:
            filename: è¾“å‡ºæ–‡ä»¶å
            frames: éŸ³é¢‘å¸§ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨æœ€è¿‘å½•åˆ¶çš„ï¼‰
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        import pyaudio
        
        if frames is None:
            frames = self.frames
        
        if not frames:
            raise ValueError("æ²¡æœ‰å½•éŸ³æ•°æ®å¯ä¿å­˜")
        
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(self.channels)
            wf.setsampwidth(self.pyaudio.get_sample_size(self.format_type))
            wf.setframerate(self.sample_rate)
            wf.writeframes(b''.join(frames))
        
        return filename
    
    def close(self):
        """é‡Šæ”¾èµ„æº"""
        if self.stream:
            self.stream.close()
        self.pyaudio.terminate()


# ============================================================================
# ä»åŸæ–‡ä»¶å¯¼å…¥çš„è¯„åˆ†æ¨¡å—ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ============================================================================

import torch
import torch.nn as nn
import torchaudio


@dataclass
class ScoringResult:
    """è¯„åˆ†ç»“æœæ•°æ®ç±»"""
    audio_score: float
    transcribed_text: str
    llm_score: float
    audio_score_detail: Optional[Dict[str, Any]] = None
    llm_score_detail: Optional[str] = None
    
    def to_dict(self) -> Dict:
        return {
            "audio_score": self.audio_score,
            "transcribed_text": self.transcribed_text,
            "llm_score": self.llm_score,
            "audio_score_detail": self.audio_score_detail,
            "llm_score_detail": self.llm_score_detail
        }


class CNN_LSTM_Regressor(nn.Module):
    """CNN-LSTM å›å½’æ¨¡å‹"""
    
    def __init__(self, 
                 n_mels: int = 128,
                 cnn_channels: list = None,
                 lstm_hidden_size: int = 128,
                 lstm_num_layers: int = 2,
                 lstm_dropout: float = 0.3,
                 fc_hidden_size: int = 128,
                 fc_dropout: float = 0.3):
        super(CNN_LSTM_Regressor, self).__init__()
        
        if cnn_channels is None:
            cnn_channels = [32, 64, 128]
        
        self.cnn_layers = nn.ModuleList()
        in_channels = 1
        for out_channels in cnn_channels:
            self.cnn_layers.append(
                nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(),
                    nn.MaxPool2d(kernel_size=2, stride=2)
                )
            )
            in_channels = out_channels
        
        self.freq_dim_after_cnn = n_mels // (2 ** len(cnn_channels))
        self.cnn_out_features = cnn_channels[-1] * self.freq_dim_after_cnn
        
        self.lstm = nn.LSTM(
            input_size=self.cnn_out_features,
            hidden_size=lstm_hidden_size,
            num_layers=lstm_num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=lstm_dropout if lstm_num_layers > 1 else 0
        )
        
        lstm_output_size = lstm_hidden_size * 2
        
        self.fc = nn.Sequential(
            nn.Linear(lstm_output_size, fc_hidden_size),
            nn.ReLU(),
            nn.Dropout(fc_dropout),
            nn.Linear(fc_hidden_size, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for cnn_layer in self.cnn_layers:
            x = cnn_layer(x)
        
        batch, channels, freq, time = x.size()
        x = x.permute(0, 3, 1, 2)
        x = x.reshape(batch, time, channels * freq)
        
        lstm_out, _ = self.lstm(x)
        pooled = torch.mean(lstm_out, dim=1)
        score = self.fc(pooled)
        
        return score


class AudioScorer:
    """è¯­éŸ³è´¨é‡è¯„åˆ†å™¨"""
    
    def __init__(self, model_path: str, device: str = "auto"):
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        self.sample_rate = 16000
        self.max_length = 30
        self.max_samples = self.sample_rate * self.max_length
        self.n_mels = 128
        
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=self.sample_rate,
            n_fft=400,
            hop_length=160,
            n_mels=self.n_mels
        )
        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()
        
        self.model = self._load_model(model_path)
    
    def _load_model(self, model_path: str) -> CNN_LSTM_Regressor:
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        model = CNN_LSTM_Regressor()
        checkpoint = torch.load(model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(self.device)
        model.eval()
        return model
    
    def preprocess_audio(self, audio_path: str) -> torch.Tensor:
        waveform, sr = torchaudio.load(audio_path)
        
        if sr != self.sample_rate:
            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
            waveform = resampler(waveform)
        
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        if waveform.shape[1] > self.max_samples:
            waveform = waveform[:, :self.max_samples]
        else:
            pad_length = self.max_samples - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, pad_length))
        
        mel_spec = self.mel_transform(waveform)
        mel_spec_db = self.amplitude_to_db(mel_spec)
        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)
        
        return mel_spec_db
    
    def score(self, audio_path: str) -> Tuple[float, Dict]:
        mel_spec_db = self.preprocess_audio(audio_path)
        mel_spec_db = mel_spec_db.unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            output = self.model(mel_spec_db)
            score = output.item()
        
        score = max(1.0, min(6.0, score))
        return score, {"raw_score": output.item(), "clamped_score": score}


class SpeechToText:
    """è¯­éŸ³è½¬æ–‡å­—æ¨¡å—"""
    
    def __init__(self, model_path: str, device: str = "auto"):
        self.model_path = model_path
        
        if device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
                self.compute_type = "float16"
            else:
                self.device = "cpu"
                self.compute_type = "int8"
        else:
            self.device = device
            self.compute_type = "float16" if device != "cpu" else "int8"
        
        self.model = self._load_model()
    
    def _load_model(self):
        from faster_whisper import WhisperModel
        
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"Whisper æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        return WhisperModel(
            model_size_or_path=self.model_path,
            device=self.device,
            compute_type=self.compute_type
        )
    
    def transcribe(self, audio_path: str) -> str:
        segments, _ = self.model.transcribe(
            audio_path,
            language="en",
            beam_size=4,
            vad_filter=True
        )
        return " ".join([seg.text.strip() for seg in segments]).strip()


class LLMScorer:
    """LLM å†…å®¹è¯„åˆ†å™¨"""
    
    PROMPT_TEMPLATE = """You are an expert English evaluator. Score the student's answer from 0 to 10.

Criteria:
1. Relevance to the question
2. Correct use of information from reference
3. Completeness and clarity
4. Use of important keywords (not simple copying)

Question: {question}
Reference: {reference}
Student Answer: {answer}

Provide:
1. Score (0-10)
2. Brief feedback in Chinese (2-3 sentences)

Format your response as:
åˆ†æ•°: [number]
è¯„ä»·: [feedback]"""
    
    def __init__(self, host: str = "http://localhost:11434", model: str = "qwen3:8b"):
        self.host = host.rstrip("/")
        self.model = model
    
    def score(self, question: str, reference: str, answer: str) -> Tuple[float, str]:
        import requests
        
        prompt = self.PROMPT_TEMPLATE.format(
            question=question.strip(),
            reference=reference.strip(),
            answer=answer.strip()
        )
        
        try:
            r = requests.post(
                f"{self.host}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0, "num_predict": 500}
                },
                timeout=180
            )
            r.raise_for_status()
            raw_output = r.json().get("response", "").strip()
            
            # æå–åˆ†æ•°
            m = re.search(r"åˆ†æ•°[ï¼š:]\s*(\d+(?:\.\d+)?)", raw_output)
            if m:
                score = float(m.group(1))
                score = max(0, min(10, score))
            else:
                m = re.search(r"\b(10(?:\.0+)?|[0-9](?:\.[0-9]+)?)\b", raw_output)
                score = float(m.group(1)) if m else 5.0
            
            return score, raw_output
            
        except Exception as e:
            return 5.0, f"è¯„åˆ†å¤±è´¥: {e}"


# ============================================================================
# äº¤äº’å¼ä¸»ç¨‹åº
# ============================================================================

class InteractiveSpeechScoring:
    """äº¤äº’å¼è¯­éŸ³è¯„åˆ†ç³»ç»Ÿ"""
    
    def __init__(self, config: dict = None):
        """
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹è·¯å¾„ç­‰
        """
        self.config = config or {}
        self.audio_scorer = None
        self.speech_to_text = None
        self.llm_scorer = None
        self.recorder = None
    
    def print_header(self):
        """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
        print("\n" + "="*60)
        print("        ğŸ“ äº¤äº’å¼è‹±è¯­å£è¯­è¯„åˆ†ç³»ç»Ÿ")
        print("="*60)
        print("\næœ¬ç³»ç»Ÿå°†å¯¹æ‚¨çš„å£è¯­å›ç­”è¿›è¡Œä»¥ä¸‹è¯„ä¼°ï¼š")
        print("  1. è¯­éŸ³è´¨é‡è¯„åˆ† (1.0-6.0)")
        print("  2. è¯­éŸ³è½¬æ–‡å­—")
        print("  3. å†…å®¹è¯„åˆ† (0-10)")
        print("\n" + "-"*60)
    
    def get_input_files(self) -> Tuple[str, str]:
        """è·å–è¾“å…¥æ–‡ä»¶è·¯å¾„"""
        print("\nğŸ“ è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰ï¼š\n")
        
        default_transcript = self.config.get("default_transcript", "test_data/å¬åŠ›åŸæ–‡.txt")
        default_task = self.config.get("default_task", "test_data/ä»»åŠ¡æè¿°.txt")
        
        # å¬åŠ›åŸæ–‡
        while True:
            transcript_path = input(f"å¬åŠ›åŸæ–‡æ–‡ä»¶è·¯å¾„ [{default_transcript}]: ").strip()
            if not transcript_path:
                transcript_path = default_transcript
                print(f"  â†’ ä½¿ç”¨é»˜è®¤è·¯å¾„: {transcript_path}")
            
            if os.path.exists(transcript_path):
                break
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")
        
        # ä»»åŠ¡è¦æ±‚
        while True:
            task_path = input(f"ä»»åŠ¡è¦æ±‚æ–‡ä»¶è·¯å¾„ [{default_task}]: ").strip()
            if not task_path:
                task_path = default_task
                print(f"  â†’ ä½¿ç”¨é»˜è®¤è·¯å¾„: {task_path}")
            
            if os.path.exists(task_path):
                break
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")
        
        return transcript_path, task_path
    
    def read_file(self, path: str) -> str:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        for enc in ("utf-8", "utf-8-sig", "gbk"):
            try:
                with open(path, "r", encoding=enc) as f:
                    return f.read().strip()
            except UnicodeDecodeError:
                continue
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read().strip()
    
    def display_question(self, transcript: str, task: str):
        """æ˜¾ç¤ºé¢˜ç›®"""
        print("\n" + "="*60)
        print("ğŸ“ é¢˜ç›®ä¿¡æ¯")
        print("="*60)
        
        print("\nã€å¬åŠ›åŸæ–‡ã€‘")
        print("-"*40)
        # æ˜¾ç¤ºåŸæ–‡ï¼ˆå¦‚æœå¤ªé•¿åˆ™æˆªæ–­ï¼‰
        if len(transcript) > 500:
            print(transcript[:500] + "...")
            print(f"\n(åŸæ–‡å…± {len(transcript)} å­—ç¬¦ï¼Œå·²æˆªæ–­æ˜¾ç¤º)")
        else:
            print(transcript)
        
        print("\nã€ä»»åŠ¡è¦æ±‚ã€‘")
        print("-"*40)
        print(task)
        
        print("\n" + "="*60)
    
    def play_audio_prompt(self):
        """æç¤ºæ’­æ”¾éŸ³é¢‘ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰"""
        audio_path = input("\nğŸ”Š é¢˜ç›®éŸ³é¢‘è·¯å¾„ (ç›´æ¥å›è½¦è·³è¿‡): ").strip()
        
        if audio_path and os.path.exists(audio_path):
            print(f"\nè¯·æ’­æ”¾éŸ³é¢‘æ–‡ä»¶: {audio_path}")
            print("ï¼ˆæœ¬ç³»ç»Ÿä¸æ”¯æŒè‡ªåŠ¨æ’­æ”¾ï¼Œè¯·ä½¿ç”¨ç³»ç»Ÿæ’­æ”¾å™¨ï¼‰")
            input("æ’­æ”¾å®ŒæˆåæŒ‰ Enter ç»§ç»­...")
        elif audio_path:
            print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
    
    def record_answer(self) -> str:
        """å½•åˆ¶è€ƒç”Ÿå›ç­”"""
        print("\n" + "="*60)
        print("ğŸ¤ å½•éŸ³ç¯èŠ‚")
        print("="*60)
        
        print("\nå‡†å¤‡å¥½åï¼ŒæŒ‰ Enter å¼€å§‹å½•éŸ³...")
        input()
        
        # åˆå§‹åŒ–å½•éŸ³å™¨
        self.recorder = AudioRecorder(sample_rate=16000)
        
        # å¼€å§‹å½•éŸ³
        self.recorder.start_recording()
        
        # ç­‰å¾…ç”¨æˆ·æŒ‰ Enter åœæ­¢
        input()
        
        # åœæ­¢å½•éŸ³
        frames = self.recorder.stop_recording()
        
        if not frames:
            print("âŒ å½•éŸ³å¤±è´¥ï¼Œæ²¡æœ‰æ•è·åˆ°éŸ³é¢‘")
            return None
        
        # è®¡ç®—å½•éŸ³æ—¶é•¿
        duration = len(frames) * self.recorder.chunk_size / self.recorder.sample_rate
        print(f"\nâœ… å½•éŸ³å®Œæˆï¼æ—¶é•¿: {duration:.1f} ç§’")
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.gettempdir()
        timestamp = int(time.time())
        audio_path = os.path.join(temp_dir, f"recording_{timestamp}.wav")
        
        self.recorder.save_to_wav(audio_path, frames)
        print(f"ğŸ“ å½•éŸ³å·²ä¿å­˜: {audio_path}")
        
        self.recorder.close()
        
        return audio_path
    
    def initialize_models(self):
        """åˆå§‹åŒ–è¯„åˆ†æ¨¡å‹"""
        print("\n" + "="*60)
        print("âš™ï¸ åˆå§‹åŒ–è¯„åˆ†æ¨¡å‹")
        print("="*60)
        
        # ç›´æ¥ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        wav2vec_path = self.config.get("wav2vec_model_path")
        whisper_path = self.config.get("whisper_model_path")
        ollama_host = self.config.get("ollama_host", "http://localhost:11434")
        ollama_model = self.config.get("ollama_model", "qwen3:8b")
        
        print(f"\næ¨¡å‹é…ç½®:")
        print(f"  wav2vec:  {wav2vec_path}")
        print(f"  whisper:  {whisper_path}")
        print(f"  ollama:   {ollama_host} / {ollama_model}")
        print()
        
        print("[1/3] åŠ è½½è¯­éŸ³è¯„åˆ†æ¨¡å‹...")
        try:
            self.audio_scorer = AudioScorer(wav2vec_path)
            print("  âœ… è¯­éŸ³è¯„åˆ†æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
            self.audio_scorer = None
        
        print("[2/3] åŠ è½½è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹...")
        try:
            self.speech_to_text = SpeechToText(whisper_path)
            print("  âœ… è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
            self.speech_to_text = None
        
        print("[3/3] åˆå§‹åŒ– LLM è¯„åˆ†å™¨...")
        self.llm_scorer = LLMScorer(ollama_host, ollama_model)
        print(f"  âœ… LLM è¯„åˆ†å™¨å°±ç»ª (æ¨¡å‹: {ollama_model})")
    
    def evaluate(self, audio_path: str, transcript: str, task: str) -> dict:
        """æ‰§è¡Œè¯„ä¼°"""
        print("\n" + "="*60)
        print("ğŸ“Š æ­£åœ¨è¯„ä¼°...")
        print("="*60)
        
        result = {
            "audio_score": None,
            "transcribed_text": "",
            "llm_score": None,
            "llm_feedback": ""
        }
        
        # Step 1: è¯­éŸ³è´¨é‡è¯„åˆ†
        print("\n[1/3] è¯­éŸ³è´¨é‡è¯„åˆ†...")
        if self.audio_scorer:
            try:
                score, detail = self.audio_scorer.score(audio_path)
                result["audio_score"] = score
                print(f"  âœ… è¯­éŸ³è¯„åˆ†: {score:.2f} / 6.0")
            except Exception as e:
                print(f"  âŒ è¯„åˆ†å¤±è´¥: {e}")
        else:
            print("  âš ï¸ è¯­éŸ³è¯„åˆ†æ¨¡å—æœªåŠ è½½")
        
        # Step 2: è¯­éŸ³è½¬æ–‡å­—
        print("\n[2/3] è¯­éŸ³è½¬æ–‡å­—...")
        if self.speech_to_text:
            try:
                text = self.speech_to_text.transcribe(audio_path)
                result["transcribed_text"] = text
                print(f"  âœ… è½¬å†™å®Œæˆ ({len(text)} å­—ç¬¦)")
                print(f"\n  è½¬å†™ç»“æœï¼š")
                print(f"  {text[:200]}{'...' if len(text) > 200 else ''}")
            except Exception as e:
                print(f"  âŒ è½¬å†™å¤±è´¥: {e}")
        else:
            print("  âš ï¸ è¯­éŸ³è½¬æ–‡å­—æ¨¡å—æœªåŠ è½½")
        
        # Step 3: LLM å†…å®¹è¯„åˆ†
        print("\n[3/3] LLM å†…å®¹è¯„åˆ†...")
        if self.llm_scorer and result["transcribed_text"]:
            try:
                score, feedback = self.llm_scorer.score(task, transcript, result["transcribed_text"])
                result["llm_score"] = score
                result["llm_feedback"] = feedback
                print(f"  âœ… å†…å®¹è¯„åˆ†: {score:.1f} / 10.0")
            except Exception as e:
                print(f"  âŒ è¯„åˆ†å¤±è´¥: {e}")
        elif not result["transcribed_text"]:
            print("  âš ï¸ æ— è½¬å†™æ–‡æœ¬ï¼Œè·³è¿‡å†…å®¹è¯„åˆ†")
        
        return result
    
    def display_result(self, result: dict):
        """æ˜¾ç¤ºæœ€ç»ˆç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“‹ è¯„ä¼°ç»“æœ")
        print("="*60)
        
        print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        
        # è¯­éŸ³è´¨é‡è¯„åˆ†
        if result["audio_score"] is not None:
            score = result["audio_score"]
            bar = "â–ˆ" * int(score) + "â–‘" * (6 - int(score))
            print(f"â”‚ è¯­éŸ³è´¨é‡è¯„åˆ†: {score:.2f} / 6.0  [{bar}]")
        else:
            print(f"â”‚ è¯­éŸ³è´¨é‡è¯„åˆ†: æœªè¯„ä¼°")
        
        # å†…å®¹è¯„åˆ†
        if result["llm_score"] is not None:
            score = result["llm_score"]
            bar = "â–ˆ" * int(score) + "â–‘" * (10 - int(score))
            print(f"â”‚ å†…å®¹è¯„åˆ†:     {score:.1f} / 10.0 [{bar}]")
        else:
            print(f"â”‚ å†…å®¹è¯„åˆ†:     æœªè¯„ä¼°")
        
        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
        # è½¬å†™æ–‡æœ¬
        if result["transcribed_text"]:
            print("\nã€æ‚¨çš„å›ç­”ï¼ˆè½¬å†™æ–‡æœ¬ï¼‰ã€‘")
            print("-"*40)
            print(result["transcribed_text"])
        
        # LLM åé¦ˆ
        if result["llm_feedback"]:
            print("\nã€è¯„ä»·ä¸å»ºè®®ã€‘")
            print("-"*40)
            # æå–è¯„ä»·éƒ¨åˆ†
            feedback = result["llm_feedback"]
            if "è¯„ä»·:" in feedback or "è¯„ä»·ï¼š" in feedback:
                match = re.search(r"è¯„ä»·[ï¼š:]\s*(.+)", feedback, re.DOTALL)
                if match:
                    print(match.group(1).strip())
                else:
                    print(feedback)
            else:
                print(feedback)
        
        print("\n" + "="*60)
    
    def run(self):
        """è¿è¡Œäº¤äº’å¼è¯„åˆ†ç³»ç»Ÿ"""
        self.print_header()
        
        # æ£€æŸ¥ä¾èµ–
        if not check_dependencies():
            return
        
        try:
            # è·å–è¾“å…¥æ–‡ä»¶
            transcript_path, task_path = self.get_input_files()
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            transcript = self.read_file(transcript_path)
            task = self.read_file(task_path)
            
            # æ˜¾ç¤ºé¢˜ç›®
            self.display_question(transcript, task)
            
            # æ’­æ”¾éŸ³é¢‘æç¤º
            self.play_audio_prompt()
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.initialize_models()
            
            # å½•åˆ¶å›ç­”
            audio_path = self.record_answer()
            
            if not audio_path:
                print("\nâŒ å½•éŸ³å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
                return
            
            # è¯„ä¼°
            result = self.evaluate(audio_path, transcript, task)
            
            # æ˜¾ç¤ºç»“æœ
            self.display_result(result)
            
            # è¯¢é—®æ˜¯å¦ä¿å­˜
            save = input("\næ˜¯å¦ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼Ÿ(y/n): ").strip().lower()
            if save == 'y':
                output_path = f"result_{int(time.time())}.json"
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if audio_path and os.path.exists(audio_path) and "temp" in audio_path.lower():
                try:
                    os.remove(audio_path)
                except:
                    pass
            
            print("\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ ğŸ‘‹\n")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


# ============================================================================
# å‘½ä»¤è¡Œå…¥å£
# ============================================================================

def main():
    import argparse
    
    # ========== é»˜è®¤é…ç½®ï¼ˆå¯ç›´æ¥ä¿®æ”¹æ­¤å¤„ï¼‰ ==========
    DEFAULT_WAV2VEC_MODEL = "/root/autodl-tmp/models/checkpoints/best_model.pth"
    DEFAULT_WHISPER_MODEL = "/root/autodl-tmp/models/whisper-base.en"
    DEFAULT_OLLAMA_HOST = "http://localhost:11434"
    DEFAULT_OLLAMA_MODEL = "qwen3:8b"
    DEFAULT_TRANSCRIPT = "test_data/å¬åŠ›åŸæ–‡.txt"
    DEFAULT_TASK = "test_data/ä»»åŠ¡æè¿°.txt"
    # ================================================
    
    parser = argparse.ArgumentParser(description="äº¤äº’å¼è¯­éŸ³è¯„åˆ†ç³»ç»Ÿ")
    parser.add_argument("--wav2vec-model", type=str, default=DEFAULT_WAV2VEC_MODEL,
                        help="wav2vec æ¨¡å‹è·¯å¾„")
    parser.add_argument("--whisper-model", type=str, default=DEFAULT_WHISPER_MODEL,
                        help="Whisper æ¨¡å‹è·¯å¾„")
    parser.add_argument("--ollama-host", type=str, default=DEFAULT_OLLAMA_HOST,
                        help="Ollama æœåŠ¡åœ°å€")
    parser.add_argument("--ollama-model", type=str, default=DEFAULT_OLLAMA_MODEL,
                        help="Ollama æ¨¡å‹åç§°")
    parser.add_argument("--transcript", type=str, default=DEFAULT_TRANSCRIPT,
                        help="å¬åŠ›åŸæ–‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--task", type=str, default=DEFAULT_TASK,
                        help="ä»»åŠ¡è¦æ±‚æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    config = {
        "wav2vec_model_path": args.wav2vec_model,
        "whisper_model_path": args.whisper_model,
        "ollama_host": args.ollama_host,
        "ollama_model": args.ollama_model,
        "default_transcript": args.transcript,
        "default_task": args.task
    }
    
    system = InteractiveSpeechScoring(config)
    system.run()


if __name__ == "__main__":
    main()